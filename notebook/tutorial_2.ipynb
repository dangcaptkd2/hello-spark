{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e2dcffa-8cdc-42de-88b6-86ef4c767c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, Word2Vec\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, CountVectorizer\n",
    "from pyspark.ml.feature import Binarizer, PCA, PolynomialExpansion\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler, MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import ChiSqSelector, UnivariateFeatureSelector\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1fe351-40d4-450b-97e0-c0b6ce162d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/24 14:30:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/24 14:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/03/24 14:30:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275369bc-ad7b-45ab-8e98-4eb09ccca4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f06c42571c0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff40f1-fc4f-415b-8020-d993c318d4ab",
   "metadata": {},
   "source": [
    "# Feature extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49d21ef-faf0-47af-9d92-af99a4ea2966",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafea1bd-7248-4a37-a02e-ebb02957abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6adcba21-183e-4d37-9b54-d3da7da5bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "wordsData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bac9eb6-d713-41cb-b0c6-5726dd524ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[6,8,13,16],[...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(20,[0,2,7,13,15,...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c91a4e-79f8-4797-9a2e-5dd9b438e409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(20, {6: 1.0, 8: 1.0, 13: 1.0, 16: 2.0})),\n",
       " Row(rawFeatures=SparseVector(20, {0: 1.0, 2: 1.0, 7: 2.0, 13: 1.0, 15: 1.0, 16: 1.0})),\n",
       " Row(rawFeatures=SparseVector(20, {3: 1.0, 4: 1.0, 6: 1.0, 11: 1.0, 19: 1.0}))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizedData.select('rawFeatures').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7b54758-a00b-4e9e-980a-48afe3a5a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[6,8,13,16],[...|\n",
      "|  0.0|(20,[0,2,7,13,15,...|\n",
      "|  1.0|(20,[3,4,6,11,19]...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0ffc849-46dc-49d6-9562-d8a3a39d4b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(20, {6: 1.0, 8: 1.0, 13: 1.0, 16: 2.0})),\n",
       " Row(rawFeatures=SparseVector(20, {0: 1.0, 2: 1.0, 7: 2.0, 13: 1.0, 15: 1.0, 16: 1.0})),\n",
       " Row(rawFeatures=SparseVector(20, {3: 1.0, 4: 1.0, 6: 1.0, 11: 1.0, 19: 1.0}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.select('rawFeatures').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aeaffb-dbb4-4672-b4fd-18a80c0655d3",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58089b9e-ead3-4c3c-84e4-bcb6c61b1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|[Hi, I, heard, ab...|\n",
      "|[I, wish, Java, c...|\n",
      "|[Logistic, regres...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input data: Each row is a bag of words from a sentence or document\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "documentDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8eda2b0-1ca6-4288-9397-64f44207ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|              result|\n",
      "+--------------------+--------------------+\n",
      "|[Hi, I, heard, ab...|[0.02524184100329...|\n",
      "|[I, wish, Java, c...|[0.01550511190933...|\n",
      "|[Logistic, regres...|[0.05185182392597...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6a76d4-f961-4879-a158-6a9e42317342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.02524184100329876,0.06749466136097908,-0.07457625474780798]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.015505111909338405,0.00871599545436246,0.0024844797860298835]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.051851823925971985,0.06089110374450684,-0.018673397600650787]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [{}] => \\nVector: {}\\n\".format(\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d9ab5-6825-4fae-b7ab-81be52a09b7f",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e0c3e7d-cbf7-44cd-878d-6900ef05499f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6eff64-fbf0-4a04-b23f-2c6ee575d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3541d3-fb12-45a7-9377-ef3da5e66a73",
   "metadata": {},
   "source": [
    "# Feature transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dda6ef-165b-4f2b-a1d0-4f45fa3a5f88",
   "metadata": {},
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bd23e-f49b-4931-9927-f7fa08063a71",
   "metadata": {},
   "source": [
    "Tokenizer vs RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbdce825-5639-49e2-8d67-f58f86d7f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00704ab4-35fe-41f3-830e-4c4d9a6a3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe4930c-1c32-4de6-aaf7-3a2872afede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdd9f823-ddd5-4d04-a1e5-822bc6c99b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e375ca-4a92-428c-8b20-cdcf767281fb",
   "metadata": {},
   "source": [
    "# stopwordsremover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a082078b-06f2-4daa-97fb-6758aa5c1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filterd             |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filterd\")\n",
    "remover.transform(sentenceData).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8438ed-70b4-4187-aa91-9c64b27acbeb",
   "metadata": {},
   "source": [
    "# n_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1131f06a-1e84-4bd9-8580-8e9ecde3f988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+\n",
      "|ngrams                                                            |\n",
      "+------------------------------------------------------------------+\n",
      "|[Hi I, I heard, heard about, about Spark]                         |\n",
      "|[I wish, wish Java, Java could, could use, use case, case classes]|\n",
      "|[Logistic regression, regression models, models are, are neat]    |\n",
      "+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ec226-06e6-4572-b60d-0f75f3d5bf96",
   "metadata": {},
   "source": [
    "# Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e53287c5-ac17-4ffd-aac7-e49df0763a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer output with Threshold = 0.5\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "print(\"Binarizer output with Threshold = {}\".format(binarizer.getThreshold()))\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484ccc3-ac92-4982-8a9d-f9ed8d3cb630",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5feda1de-65dc-4947-9a4a-56129865529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|pcaFeatures                                                 |\n",
      "+------------------------------------------------------------+\n",
      "|[1.6485728230883814,-4.0132827005162985,-1.0091435193998504]|\n",
      "|[-4.645104331781533,-1.1167972663619048,-1.0091435193998504]|\n",
      "|[-6.428880535676488,-5.337951427775359,-1.0091435193998508] |\n",
      "+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "\n",
    "result = model.transform(df).select(\"pcaFeatures\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db03a0b-9341-4ac1-ad65-cbf3da88dc92",
   "metadata": {},
   "source": [
    "# feature expansion in a polynomial space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69739a28-3231-43a7-87f9-f1cd02c1b5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+\n",
      "|features  |polyFeatures                              |\n",
      "+----------+------------------------------------------+\n",
      "|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n",
      "|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n",
      "|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n",
      "+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (Vectors.dense([2.0, 1.0]),),\n",
    "    (Vectors.dense([0.0, 0.0]),),\n",
    "    (Vectors.dense([3.0, -1.0]),)\n",
    "], [\"features\"])\n",
    "\n",
    "polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    "polyDF = polyExpansion.transform(df)\n",
    "\n",
    "polyDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aae19d-8ebd-4c38-8b09-6682a19bb1b7",
   "metadata": {},
   "source": [
    "# string indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65a3402a-087e-49ce-a3f7-9c406931f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38083e5f-e6d9-4428-aa8d-45872e2b112c",
   "metadata": {},
   "source": [
    "# index2string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f860fbd7-e080-481c-a760-5abe8c0031c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed string column category to indexed column categoryIndex\n",
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = indexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "\n",
    "print(\"Transformed string column {} to indexed column {}\".format(indexer.getInputCol(), indexer.getOutputCol()))\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5788d59c-cbec-4705-9455-c3e4cd218d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringIndexer will store labels in output column metadata\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"StringIndexer will store labels in output column metadata\\n\")\n",
    "\n",
    "converter = IndexToString(inputCol=\"categoryIndex\", outputCol=\"originalCategory\")\n",
    "converted = converter.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "569a60a5-800b-4e96-a899-4b7a15f15d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed indexed colum categoryIndex back to original string column originalCategory using labels in metadata\n",
      "+---+-------------+----------------+\n",
      "| id|categoryIndex|originalCategory|\n",
      "+---+-------------+----------------+\n",
      "|  0|          0.0|               a|\n",
      "|  1|          2.0|               b|\n",
      "|  2|          1.0|               c|\n",
      "|  3|          0.0|               a|\n",
      "|  4|          0.0|               a|\n",
      "|  5|          1.0|               c|\n",
      "+---+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed indexed colum {} back to original string column {} using labels in metadata\".format(converter.getInputCol(), converter.getOutputCol()))\n",
    "converted.select(\"id\", \"categoryIndex\", \"originalCategory\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff38127-b7d1-42ef-8bc9-fcb63335fa2e",
   "metadata": {},
   "source": [
    "# OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb242d1e-ec06-44ad-a94a-c88987827e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdce30a2-0cc4-4b10-aa86-8398539d7164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder  = OneHotEncoder(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                         outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38391db0-151d-49f7-82eb-2156ddabed5d",
   "metadata": {},
   "source": [
    "# Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed116167-cab0-40ae-ac78-5e9add89fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^1 norm\n",
      "+---+--------------+------------------+\n",
      "| id|      features|      normFeatures|\n",
      "+---+--------------+------------------+\n",
      "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
      "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
      "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
      "+---+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Normalize each Vector using L1 norm\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(dataFrame)\n",
    "print(\"Normalized using L^1 norm\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae197721-f381-4480-be4a-5fb5671578ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^inf norm\n",
      "+---+--------------+--------------+\n",
      "| id|      features|  normFeatures|\n",
      "+---+--------------+--------------+\n",
      "|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n",
      "|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n",
      "|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n",
      "+---+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using L^inf norm\n",
    "lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalized using L^inf norm\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45f85f9-4708-4a8a-b136-b235831c08c5",
   "metadata": {},
   "source": [
    "# StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dae838cc-2718-4714-ab44-7be35f8ce0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.read.format(\"libsvm\").load(\"/home/quyennt72/pyspark-tutorial/data/sample_libsvm_data.txt\")\n",
    "dataFrame.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ea002d7-566b-437b-bdb4-d197cfacc831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|      scaledFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a69ddc-59a2-495e-a489-11ced2df0701",
   "metadata": {},
   "source": [
    "# MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7871d630-0b63-448c-849f-b17c10790ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  2|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "dataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6647242-960a-42a5-b8d3-cf58ea4907d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1.000000]\n",
      "+--------------+--------------+\n",
      "|      features|scaledFeatures|\n",
      "+--------------+--------------+\n",
      "|[1.0,0.1,-1.0]|     (3,[],[])|\n",
      "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411d9b9-5a7f-4f21-ae9b-d0cf4655c29b",
   "metadata": {},
   "source": [
    "# vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "694a3a15-4505-4f71-bc1e-d2f5ead53047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de0d5b-d389-402d-ab75-5f55f1e0dcba",
   "metadata": {},
   "source": [
    "# Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522474d-1a72-4cf2-a902-32189b4fc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1.0, float(\"nan\")),\n",
    "    (2.0, float(\"nan\")),\n",
    "    (float(\"nan\"), 3.0),\n",
    "    (4.0, 4.0),\n",
    "    (5.0, 5.0)\n",
    "], [\"a\", \"b\"])\n",
    "\n",
    "imputer = Imputer(inputCols=[\"a\", \"b\"], outputCols=[\"out_a\", \"out_b\"])\n",
    "model = imputer.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd84959-b83b-4844-9a45-4586e604053d",
   "metadata": {},
   "source": [
    "# Vector selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3505dc0-8c3b-4d02-97de-08b788bc297b",
   "metadata": {},
   "source": [
    "## vector slicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c34e0dda-3f9c-4487-9f1f-eb46a5529523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|        userFeatures|     features|\n",
      "+--------------------+-------------+\n",
      "|(3,[0,1],[-2.0,2.3])|(1,[0],[2.3])|\n",
      "|      [-2.0,2.3,0.0]|        [2.3]|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorSlicer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n",
    "    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]))])\n",
    "\n",
    "slicer = VectorSlicer(inputCol=\"userFeatures\", outputCol=\"features\", indices=[1])\n",
    "\n",
    "output = slicer.transform(df)\n",
    "\n",
    "output.select(\"userFeatures\", \"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1365b1-4874-4442-99ed-33a0413dcc59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-tutorial",
   "language": "python",
   "name": "pyspark-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
